{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf10ff16",
   "metadata": {},
   "source": [
    "# Lewis Rincon Castano\n",
    "## Project: Extracting Text from Images with Tesseract and Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c35cb",
   "metadata": {},
   "source": [
    "### **Image and Output Files:** https://github.com/lericas/portfolio/tree/main/fall%202024\n",
    "### **Hugging Face Pipelines Information**: https://huggingface.co/docs/transformers/en/main_classes/pipelines\n",
    "\n",
    "### **Pros:**\n",
    "\n",
    "1. **Automated Data Extraction**: Saves time by extracting text from invoices/receipts without manual entry.\n",
    "2. **Supports Multiple Image Formats**: Compatible with various formats (PNG, JPEG, etc.).\n",
    "3. **Flexible Parsing**: Uses regex to extract common fields like phone numbers, dates, and amounts.\n",
    "4. **Structured Output**: Exports data to Excel for easy analysis and integration.\n",
    "5. **Batch Processing**: Handles multiple images in a directory efficiently.\n",
    "6. **Easily Customizable**: Modular design allows for easy modifications and field additions.\n",
    "\n",
    "### **Cons:**\n",
    "\n",
    "1. **Accuracy Depends on Image Quality**: Poorly scanned or low-resolution images result in inaccurate text extraction.\n",
    "2. **Limited Context Understanding**: Relies on simple rules, lacking advanced interpretation of document layouts.\n",
    "3. **Weak OCR on Complex Text**: Struggles with non-standard fonts or handwritten text.\n",
    "4. **Rigid Field Extraction**: Fixed logic may not work with varying formats (e.g., international layouts).\n",
    "5. **Lack of Adaptability**: Assumes static positions for fields, requiring manual adjustments for different layouts.\n",
    "6. **Security Risks for Model Using Hugging Transformers**: During our testing, we got a warning about torch.load with weights_only=False indicates a potential security risk due to the execution of arbitrary code during unpickling. This could lead to vulnerabilities if untrusted model files are used.\n",
    "\n",
    "### **Limitations:**\n",
    "\n",
    "1. **Dependent on Image Quality**: Blurry or poorly lit images affect OCR performance.\n",
    "2. **No Advanced NLP**: Can't handle complex invoice formats or variations in terminology.\n",
    "3. **Locale-Specific Formats**: May not handle international formats (dates, currency, addresses) without customization.\n",
    "4. **No Handwriting Support**: Unable to process handwritten content effectively.\n",
    "5. **Lack of Data Validation**: No error handling or post-processing to verify extracted data.\n",
    "6. **Static Rule-Based Parsing**: Needs frequent adjustments for non-standard or varying document layouts.\n",
    "\n",
    "### **Conclusions:** \n",
    "The code below explains two different methods to extract text from images using Tesseract and Hugging Face. Both methods produced different results; for example, Tesseract performed slightly better at displaying total amounts, while the Hugging Face model provided more accurate address information and item descriptions. These results are not conclusive because each receipt is unique in its content and format. The code included below serves as a guide for this practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc6c7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5775bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\lewis\\\\Desktop\\\\Deep Learning Project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc5b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install missing packages:\n",
    "# !pip install [package name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70400b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lewis\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\lewis\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\lewis\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b73f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Tesseract executable path (adjust for your OS)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Update this for your system\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"Extract text from an image using Tesseract OCR.\"\"\"\n",
    "    img = Image.open(image_path)  # Open the image\n",
    "    text = pytesseract.image_to_string(img)  # Perform OCR to extract text\n",
    "    return text\n",
    "\n",
    "def parse_extracted_text(text):\n",
    "    \"\"\"Parse the extracted text and structure it into the desired fields.\"\"\"\n",
    "    \n",
    "    # Use regular expressions or keyword searches to extract specific details\n",
    "    company_name = address = phone_number = payment_amount = date = item_description = other = None\n",
    "\n",
    "    # Extract company name (assuming the company name is in the first line of text)\n",
    "    lines = text.split('\\n')\n",
    "    if lines:\n",
    "        company_name = lines[0].strip()  # Assuming company name is at the top of the document\n",
    "\n",
    "    # Extract phone number (using regex to match phone number patterns)\n",
    "    phone_match = re.search(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)\n",
    "    if phone_match:\n",
    "        phone_number = phone_match.group()\n",
    "\n",
    "    # Extract address (this could be complex, depending on format)\n",
    "    # Assume the address follows the company name\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'address' in line.lower():\n",
    "            address = lines[i + 1].strip()  # Assuming address follows after the line containing \"address\"\n",
    "\n",
    "    # Extract payment amount (assuming it has a $ or some currency symbol)\n",
    "    payment_match = re.search(r'[\\$\\€\\£]\\s*\\d+(?:,\\d{3})*(?:\\.\\d{2})?', text)\n",
    "    if payment_match:\n",
    "        payment_amount = payment_match.group()\n",
    "\n",
    "    # Extract date (looking for common date formats like MM/DD/YYYY or DD/MM/YYYY)\n",
    "    date_match = re.search(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', text)\n",
    "    if date_match:\n",
    "        date = date_match.group()\n",
    "\n",
    "    # Extract item description (this could vary depending on format, let's assume it's after a keyword like \"description\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'description' in line.lower():\n",
    "            item_description = lines[i + 1].strip()  # Assuming the description follows after a line mentioning \"description\"\n",
    "\n",
    "    # Extract other information (could be any unclassified info for now)\n",
    "    other = \"\\n\".join(lines)  # For now, just adding the remaining text\n",
    "\n",
    "    # Return structured data as a dictionary\n",
    "    return {\n",
    "        'Company Name': company_name,\n",
    "        'Address': address,\n",
    "        'Phone Number': phone_number,\n",
    "        'Payment Amount': payment_amount,\n",
    "        'Date': date,\n",
    "        'Item Description': item_description,\n",
    "        'Other': other\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1eb4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: C:\\Users\\lewis\\Desktop\\Deep Learning Project\\cab receipt.png\n",
      "Processing image: C:\\Users\\lewis\\Desktop\\Deep Learning Project\\IC-Basic-Receipt-Template.png\n",
      "Processing image: C:\\Users\\lewis\\Desktop\\Deep Learning Project\\magicpay.jpeg\n",
      "Processing image: C:\\Users\\lewis\\Desktop\\Deep Learning Project\\receipt-template-us-classic-white-750px.png\n",
      "Processing image: C:\\Users\\lewis\\Desktop\\Deep Learning Project\\restaurant.jpg\n",
      "Extracted data has been saved to: C:\\Users\\lewis\\Desktop\\Deep Learning Project\\extracted_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "def process_images_in_directory(directory):\n",
    "    \"\"\"Process all images in the directory and save the structured data to an Excel file.\"\"\"\n",
    "    data = []  # List to store parsed data for each image\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing image: {image_path}\")\n",
    "\n",
    "            # Step 1: Extract text from the image\n",
    "            extracted_text = extract_text_from_image(image_path)\n",
    "\n",
    "            # Step 2: Parse the extracted text into fields\n",
    "            parsed_data = parse_extracted_text(extracted_text)\n",
    "            parsed_data['Image Name'] = filename  # Add image file name for reference\n",
    "\n",
    "            # Append the structured data to the list\n",
    "            data.append(parsed_data)\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    excel_output_path = os.path.join(directory, 'extracted_data.xlsx')\n",
    "    df.to_excel(excel_output_path, index=False)\n",
    "\n",
    "    print(f\"Extracted data has been saved to: {excel_output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your working directory path (where your images are located)\n",
    "    working_directory = os.getcwd()  # This gets the current working directory\n",
    "    \n",
    "    # Process all images in the directory and save results to Excel\n",
    "    process_images_in_directory(working_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49d1d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install easyocr pandas openpyxl transformers or other libraries if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5d1519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import easyocr\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress Hugging Face cache symlink warning\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "# Force torch.load to be more secure (weights_only=True) as recommended\n",
    "def safe_torch_load(path, map_location):\n",
    "    return torch.load(path, map_location=map_location, weights_only=True)\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e078956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Hugging Face pipeline for text extraction\n",
    "nlp_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"Extract text from an image using EasyOCR.\"\"\"\n",
    "    result = reader.readtext(image_path)\n",
    "    text = ' '.join([res[1] for res in result])\n",
    "    return text\n",
    "\n",
    "def extract_field(text, question):\n",
    "    \"\"\"Use NLP model to extract specific information from the text.\"\"\"\n",
    "    result = nlp_pipeline(question=question, context=text)\n",
    "    return result['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e8b214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data has been saved to: C:\\Users\\lewis\\Desktop\\Deep Learning Project\\extract_data_with_deep_learning.xlsx\n"
     ]
    }
   ],
   "source": [
    "def process_images_in_directory(directory):\n",
    "    \"\"\"Process images, extract text with EasyOCR, query NLP model, and save to Excel.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            extracted_text = extract_text_from_image(image_path)\n",
    "            \n",
    "            # Extract specific fields using NLP\n",
    "            company_name = extract_field(extracted_text, \"What is the company name?\")\n",
    "            address = extract_field(extracted_text, \"What is the address?\")\n",
    "            phone_number = extract_field(extracted_text, \"What is the phone number?\")\n",
    "            payment_amount = extract_field(extracted_text, \"What is the payment amount?\")\n",
    "            date = extract_field(extracted_text, \"What is the date?\")\n",
    "            item_description = extract_field(extracted_text, \"What is the item description?\")\n",
    "            other = extract_field(extracted_text, \"What other information is available?\")\n",
    "\n",
    "            # Append results to the list\n",
    "            data.append({\n",
    "                'Image Name': filename,\n",
    "                'Company Name': company_name,\n",
    "                'Address': address,\n",
    "                'Phone Number': phone_number,\n",
    "                'Payment Amount': payment_amount,\n",
    "                'Date': date,\n",
    "                'Item Description': item_description,\n",
    "                'Other': other\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame and save to Excel\n",
    "    df = pd.DataFrame(data)\n",
    "    excel_output_path = os.path.join(directory, 'extract_data_with_deep_learning.xlsx')\n",
    "    df.to_excel(excel_output_path, index=False)\n",
    "    print(f\"Extracted data has been saved to: {excel_output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    working_directory = os.getcwd()  # Set the working directory\n",
    "    process_images_in_directory(working_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
